# -*- coding: utf-8 -*-
"""heartdiseaseprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o3gw2OroW5SX_NA0AjPjnGNuJ-HhC8Vd
"""

# R178512E ANESU CHITSIKU
# R178527F RUMBIDZAI MUSENDO
# R178495N WAYNE MAISENI

#HEART DISEASE PREDICTION MODEL

# Import libraries
import numpy as np
import pandas as pd
import tensorflow as tf

# Importing dataset
from google.colab import drive
drive.mount('/content/drive')

#Importing the dataset
dataset=pd.read_csv('/content/drive/My Drive/KBS Assignment1/heart.csv')

dataset.head(20)

dataset.info()

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline
from matplotlib import pyplot as plt

f = sns.countplot(x='target', data=dataset)
f.set_title("Heart disease distribution")
f.set_xticklabels(['No Heart disease', 'Heart Disease'])
plt.xlabel("");

f = sns.countplot(x='target', data=dataset, hue='sex')
plt.legend(['Female', 'Male'])
f.set_title("Heart disease by gender")
f.set_xticklabels(['No Heart disease', 'Heart Disease'])
plt.xlabel("");

heat_map = sns.heatmap(dataset.corr(method='pearson'), annot=True, fmt='.2f', linewidths=2)
heat_map.set_xticklabels(heat_map.get_xticklabels(), rotation=45);
plt.rcParams["figure.figsize"] = (50,50)

X = dataset.iloc[:, 3:-1].values
y = dataset.iloc[:, -1].values

# Commented out IPython magic to ensure Python compatibility.
# importing pandas profiling and warning filters
import pandas_profiling as pp
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

#Splitting the dataset into the Training set and Test set for validation
#Normalization, scaling and transformation
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Normalization, scaling and transformation
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#creation of an Artificial Neural Network as Deep Learning Model
#initialization
ann = tf.keras.models.Sequential()
#Addition of the input layer and the first hidden layer, second hidden layer and the outer layer
ann.add(tf.keras.layers.Dense(units=8, activation='relu'))
ann.add(tf.keras.layers.Dense(units=8, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

#Model Compilation
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

#training the model
ann.fit(X_train, y_train, batch_size = 32, epochs = 75)

#Visualization of the rules, trends or patterns
#Predicting the Test set results without supervision thus unsupervised learning
#this is an implemententation of deep learning neural network using ANN
y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

#making a confusion matrix inorder to describe the performance of the model
#Model evaluation technique
#our prediction was based on the test data and its accuracy
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
print("Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred)*100))

from sklearn.naive_bayes import MultinomialNB
multiNB = MultinomialNB()

multiNB.fit(y_pred, y_test)
accuracy5 = multiNB.score(y_pred, y_test)
print('Multinomial NB Accuracy -->',((accuracy5)*100))